# Lexer Implementation Documentation

## Table of Contents

- [Overview](#overview)
- [Data Structures](#data-structures)
- [Initialization](#initialization)
- [Core Functions](#core-functions)
- [Character Reading](#character-reading)
- [Token Generation](#token-generation)
- [Identifier Processing](#identifier-processing)
- [Number Processing](#number-processing)
- [String Processing](#string-processing)
- [Whitespace and Comments](#whitespace-and-comments)
- [Unicode Handling](#unicode-handling)
- [Operator Recognition](#operator-recognition)
- [Position Tracking](#position-tracking)
- [Complete Examples](#complete-examples)
- [Testing Strategies](#testing-strategies)
- [Performance Considerations](#performance-considerations)

---

## Overview

The **Lexer** (lexical analyzer) is the first phase of the Bhasa language interpreter. It transforms a stream of characters (source code) into a stream of tokens that can be consumed by the parser.

### Lexical Analysis Process

```
Source Code ‚Üí Lexer ‚Üí Token Stream ‚Üí Parser ‚Üí AST
```

**Example:**
```
Input:  "‡¶ß‡¶∞‡¶ø x = ‡ß´;"
        ‚Üì
Lexer:  [LET, IDENT, ASSIGN, INT, SEMICOLON, EOF]
        ‚Üì
Parser: AST (Let Statement Node)
```

### Design Principles

1. **Single Responsibility**: Only converts characters to tokens
2. **Single Pass**: Reads input once, left-to-right
3. **No Backtracking**: Peek-ahead for lookahead
4. **Unicode First**: Full Bengali script support
5. **Position Aware**: Track line/column for errors

---

## Data Structures

### Lexer Structure

```go
type Lexer struct {
    input        []rune  // Input as rune slice for Unicode support
    position     int     // Current position in input (points to current char)
    readPosition int     // Next reading position (after current char)
    ch           rune    // Current character under examination
    line         int     // Current line number (1-indexed)
    column       int     // Current column number (0-indexed initially)
}
```

#### Field Descriptions

| Field | Type | Purpose | Example State |
|-------|------|---------|---------------|
| `input` | `[]rune` | Source code as Unicode code points | `['‡¶ß', '‡¶∞', '‡¶ø', ' ', 'x']` |
| `position` | `int` | Index of current character | `0` |
| `readPosition` | `int` | Index of next character to read | `1` |
| `ch` | `rune` | Current character being examined | `'‡¶ß'` |
| `line` | `int` | Current line (starts at 1) | `1` |
| `column` | `int` | Current column (increments with each char) | `1` |

#### Why []rune Instead of string?

**Problem with strings:**
```go
s := "‡¶ß‡¶∞‡¶ø"
len(s)     // Returns 9 (bytes), not 3 (characters)
s[0]       // Returns 224 (byte), not '‡¶ß' (character)
```

**Solution with []rune:**
```go
r := []rune("‡¶ß‡¶∞‡¶ø")
len(r)     // Returns 3 (characters)
r[0]       // Returns '‡¶ß' (Unicode code point U+09A7)
```

### Token Structure

```go
type Token struct {
    Type    TokenType  // Token type (LET, IDENT, INT, etc.)
    Literal string     // Actual text from source
    Line    int        // Line number where token starts
    Column  int        // Column number where token starts
}
```

**Example Token:**
```go
token.Token{
    Type:    token.LET,
    Literal: "‡¶ß‡¶∞‡¶ø",
    Line:    1,
    Column:  1,
}
```

---

## Initialization

### New() Constructor

```go
func New(input string) *Lexer {
    l := &Lexer{
        input:  []rune(input),  // Convert string to rune slice
        line:   1,              // Start at line 1
        column: 0,              // Column will be 1 after first readChar()
    }
    l.readChar()  // Initialize by reading first character
    return l
}
```

#### Initialization Process

**Step-by-step:**
```
1. Input: "‡¶ß‡¶∞‡¶ø x"
2. Convert to runes: ['‡¶ß', '‡¶∞', '‡¶ø', ' ', 'x']
3. Initial state:
   - position = 0
   - readPosition = 0
   - ch = 0 (null)
   - line = 1
   - column = 0
4. Call readChar():
   - ch = '‡¶ß'
   - position = 0
   - readPosition = 1
   - column = 1
5. Ready to tokenize!
```

**Why read first character?**
- Lexer is always "one character ahead"
- Makes NextToken() logic simpler
- Current character is always available in `l.ch`

---

## Core Functions

### NextToken() - Main Entry Point

```go
func (l *Lexer) NextToken() token.Token {
    var tok token.Token
    
    l.skipWhitespace()  // Ignore whitespace
    
    // Record position at start of token
    tokLine := l.line
    tokCol := l.column
    
    switch l.ch {
    case '=':
        if l.peekChar() == '=' {
            // Two-character operator: ==
            ch := l.ch
            l.readChar()
            tok = l.newTokenWithPos(token.EQ, string(ch)+string(l.ch))
        } else if l.peekChar() == '>' {
            // Two-character operator: =>
            ch := l.ch
            l.readChar()
            tok = l.newTokenWithPos(token.ARROW, string(ch)+string(l.ch))
        } else {
            // Single-character operator: =
            tok = l.newTokenWithPos(token.ASSIGN, string(l.ch))
        }
    // ... more cases ...
    default:
        if isLetter(l.ch) {
            // Identifier or keyword
            literal := l.readIdentifier()
            tok = token.Token{
                Type:    token.LookupIdent(literal),
                Literal: literal,
                Line:    tokLine,
                Column:  tokCol,
            }
            return tok  // Early return (readIdentifier advances position)
        } else if isDigit(l.ch) || isBengaliDigit(l.ch) {
            // Number literal
            literal := l.readNumber()
            tok = token.Token{
                Type:    token.INT,
                Literal: literal,
                Line:    tokLine,
                Column:  tokCol,
            }
            return tok  // Early return
        } else {
            tok = l.newTokenWithPos(token.ILLEGAL, string(l.ch))
        }
    }
    
    l.readChar()  // Advance to next character
    return tok
}
```

#### Token Recognition Strategy

**1. Skip Non-Tokens:**
- Whitespace (spaces, tabs, newlines)
- Comments (line comments starting with `//`)

**2. Record Position:**
- Save line and column before consuming characters
- Tokens are labeled with their starting position

**3. Classify Character:**
- **Single-char operators:** `+`, `-`, `*`, `(`, `)`, etc.
- **Multi-char operators:** `==`, `!=`, `<=`, `>=`, `&&`, `||`, `<<`, `>>`
- **Identifiers/Keywords:** Start with letter or underscore
- **Numbers:** Start with digit
- **Strings:** Start with `"`
- **EOF:** Null character (`0`)

**4. Handle Multi-Character Tokens:**
- Use `peekChar()` for lookahead
- Consume characters as needed
- Return early to avoid extra `readChar()`

---

## Character Reading

### readChar() - Consume Next Character

```go
func (l *Lexer) readChar() {
    if l.readPosition >= len(l.input) {
        l.ch = 0  // EOF represented as null
    } else {
        l.ch = l.input[l.readPosition]
    }
    l.position = l.readPosition
    l.readPosition++
    l.column++
    
    if l.ch == '\n' {
        l.line++
        l.column = 0
    }
}
```

#### Character Reading Process

**Example: Reading "‡¶ß‡¶∞‡¶ø"**
```
Initial State:
  input = ['‡¶ß', '‡¶∞', '‡¶ø']
  position = 0, readPosition = 1
  ch = '‡¶ß', line = 1, column = 1

Call readChar():
  ch = input[1] = '‡¶∞'
  position = 1, readPosition = 2
  column = 2

Call readChar():
  ch = input[2] = '‡¶ø'
  position = 2, readPosition = 3
  column = 3

Call readChar():
  readPosition (3) >= len(input) (3)
  ch = 0 (EOF)
  position = 3, readPosition = 4
  column = 4
```

#### Newline Handling

```go
if l.ch == '\n' {
    l.line++     // Increment line number
    l.column = 0 // Reset column to 0 (will be 1 after next readChar())
}
```

**Example: Multi-line Input**
```
Input: "‡¶ß‡¶∞‡¶ø x = ‡ß´;\n‡¶ß‡¶∞‡¶ø y = ‡ßß‡ß¶;"

After first line:
  line = 1, column varies

After '\n':
  line = 2, column = 0

After '‡¶ß' in second line:
  line = 2, column = 1
```

### peekChar() - Look Ahead Without Consuming

```go
func (l *Lexer) peekChar() rune {
    if l.readPosition >= len(l.input) {
        return 0  // EOF
    }
    return l.input[l.readPosition]
}
```

#### Use Cases for Peek

**1. Two-Character Operators:**
```go
// Current: '='
// Peek: '='
// Result: Token type EQ ("==")

if l.ch == '=' && l.peekChar() == '=' {
    // It's "=="
}
```

**2. Disambiguation:**
```go
// '<' can be:
//   - LT ("<")
//   - LTE ("<=")
//   - LSHIFT ("<<")

case '<':
    if l.peekChar() == '=' {
        // "<="
    } else if l.peekChar() == '<' {
        // "<<"
    } else {
        // "<"
    }
```

**3. Comment Detection:**
```go
// '/' can be:
//   - SLASH ("/")
//   - Comment ("//")

case '/':
    if l.peekChar() == '/' {
        l.skipComment()
        return l.NextToken()  // Get next token after comment
    } else {
        tok = l.newTokenWithPos(token.SLASH, string(l.ch))
    }
```

---

## Token Generation

### newTokenWithPos() - Create Token With Position

```go
func (l *Lexer) newTokenWithPos(tokenType token.TokenType, literal string) token.Token {
    return token.Token{
        Type:    tokenType,
        Literal: literal,
        Line:    l.line,
        Column:  l.column - len([]rune(literal)) + 1,
    }
}
```

#### Column Calculation

The column calculation accounts for the fact that we've already consumed the token's characters:

```
Current position: column = 5
Token literal: "==" (length 2)
Token started at: column - len("==") + 1 = 5 - 2 + 1 = 4
```

**Example:**
```
Input: "‡¶ß‡¶∞‡¶ø x == ‡ß´"
       ^^^^^^^
       123456789

When at '=' (second one):
  l.column = 9
  literal = "=="
  Token column = 9 - 2 + 1 = 8 (correct start position)
```

**Why use []rune(literal) for length?**
- `len(literal)` returns byte count, not character count
- Bengali characters are multi-byte
- `len([]rune(literal))` returns character count

**Example:**
```go
literal := "‡¶ß‡¶∞‡¶ø"
len(literal)           // 9 bytes
len([]rune(literal))   // 3 characters
```

---

## Identifier Processing

### readIdentifier() - Read Variable/Keyword Name

```go
func (l *Lexer) readIdentifier() string {
    startPos := l.position
    // Read first character (must be letter or underscore)
    for isLetter(l.ch) || isDigit(l.ch) || isBengaliDigit(l.ch) {
        l.readChar()
    }
    return string(l.input[startPos:l.position])
}
```

#### Identifier Rules

**Valid identifiers:**
- Must start with: letter or underscore
- Can contain: letters, digits, underscores
- Bengali script fully supported

**Examples:**
```
‚úì ‡¶´‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü‡ßã‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶≤    (Bengali word)
‚úì factorial      (English word)
‚úì _private       (underscore prefix)
‚úì var123         (letters + digits)
‚úì ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®_‡ßß        (Bengali + digit)
‚úì ‡¶®‡¶æ‡¶Æ_name       (mixed Bengali + English)

‚úó 123abc         (starts with digit)
‚úó my-var         (hyphen not allowed)
‚úó my.var         (dot not allowed)
```

#### Identifier Reading Process

**Example: Reading "‡¶´‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü‡ßã‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶≤"**
```
Initial: position = 0, ch = '‡¶´'

Iteration 1: isLetter('‡¶´') = true, readChar(), ch = '‡ßç'
Iteration 2: isLetter('‡ßç') = true (vowel sign), readChar(), ch = '‡¶Ø'
Iteration 3: isLetter('‡¶Ø') = true, readChar(), ch = '‡¶æ'
Iteration 4: isLetter('‡¶æ') = true (vowel sign), readChar(), ch = '‡¶ï'
Iteration 5: isLetter('‡¶ï') = true, readChar(), ch = '‡ßç'
Iteration 6: isLetter('‡ßç') = true, readChar(), ch = '‡¶ü'
... and so on ...

Stop when: ch = ' ' (space, not a letter)
Return: input[0:current_position]
```

#### Keyword Detection

After reading an identifier, check if it's a keyword:

```go
literal := l.readIdentifier()
tok = token.Token{
    Type:    token.LookupIdent(literal),  // Returns keyword type or IDENT
    Literal: literal,
    Line:    tokLine,
    Column:  tokCol,
}
```

**LookupIdent() Logic:**
```go
func LookupIdent(ident string) TokenType {
    if tok, ok := keywords[ident]; ok {
        return tok  // It's a keyword
    }
    return IDENT  // It's an identifier
}
```

**Example:**
```
"‡¶ß‡¶∞‡¶ø"        ‚Üí LET (keyword)
"‡¶´‡¶æ‡¶Ç‡¶∂‡¶®"      ‚Üí FUNCTION (keyword)
"myVariable" ‚Üí IDENT (identifier)
```

---

## Number Processing

### readNumber() - Read Integer Literal

```go
func (l *Lexer) readNumber() string {
    startPos := l.position
    for isDigit(l.ch) || isBengaliDigit(l.ch) {
        l.readChar()
    }
    result := string(l.input[startPos:l.position])
    // Convert Bengali digits to Arabic
    return token.ConvertBengaliNumber(result)
}
```

#### Number Reading Process

**Example 1: Arabic Digits "123"**
```
Initial: position = 0, ch = '1'

Iteration 1: isDigit('1') = true, readChar(), ch = '2'
Iteration 2: isDigit('2') = true, readChar(), ch = '3'
Iteration 3: isDigit('3') = true, readChar(), ch = ';'
Stop: isDigit(';') = false

Extract: input[0:3] = "123"
Convert: ConvertBengaliNumber("123") = "123"
Return: "123"
```

**Example 2: Bengali Digits "‡ßß‡ß®‡ß©"**
```
Initial: position = 0, ch = '‡ßß'

Iteration 1: isBengaliDigit('‡ßß') = true, readChar(), ch = '‡ß®'
Iteration 2: isBengaliDigit('‡ß®') = true, readChar(), ch = '‡ß©'
Iteration 3: isBengaliDigit('‡ß©') = true, readChar(), ch = ';'
Stop: isBengaliDigit(';') = false

Extract: input[0:3] = "‡ßß‡ß®‡ß©"
Convert: ConvertBengaliNumber("‡ßß‡ß®‡ß©") = "123"
Return: "123"
```

**Example 3: Mixed Digits "‡ßß2‡ß©" (unusual but supported)**
```
Initial: position = 0, ch = '‡ßß'

Read all digits: "‡ßß2‡ß©"
Convert:
  '‡ßß' ‚Üí '1'
  '2' ‚Üí '2' (already Arabic)
  '‡ß©' ‚Üí '3'
Return: "123"
```

#### Bengali to Arabic Conversion

```go
func ConvertBengaliNumber(s string) string {
    result := ""
    for _, ch := range s {
        if digit, ok := BengaliDigits[ch]; ok {
            result += string(digit)
        } else {
            result += string(ch)
        }
    }
    return result
}
```

**Conversion Map:**
```go
var BengaliDigits = map[rune]rune{
    '‡ß¶': '0', '‡ßß': '1', '‡ß®': '2', '‡ß©': '3', '‡ß™': '4',
    '‡ß´': '5', '‡ß¨': '6', '‡ß≠': '7', '‡ßÆ': '8', '‡ßØ': '9',
}
```

**Why Convert?**
- Internal representation uses integers
- Go's `strconv.Atoi()` expects Arabic digits
- Simpler for evaluation phase

---

## String Processing

### readString() - Read String Literal

```go
func (l *Lexer) readString() string {
    startPos := l.position + 1  // Skip opening quote
    for {
        l.readChar()
        if l.ch == '"' || l.ch == 0 {  // Closing quote or EOF
            break
        }
    }
    return string(l.input[startPos:l.position])
}
```

#### String Reading Process

**Example: Reading "‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã"**
```
Input: "‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã"
       ^      ^
       start  end

Initial: position = 0, ch = '"'
startPos = 0 + 1 = 1 (skip opening quote)

Iteration 1: readChar(), ch = '‡¶π'
Iteration 2: readChar(), ch = '‡ßç'
Iteration 3: readChar(), ch = '‡¶Ø'
Iteration 4: readChar(), ch = '‡¶æ'
Iteration 5: readChar(), ch = '‡¶≤'
Iteration 6: readChar(), ch = '‡ßã'
Iteration 7: readChar(), ch = '"'
Stop: ch == '"'

Extract: input[1:7] = "‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã"
Return: "‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã"
```

#### String Features

**1. Unicode Support:**
```
"‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã"        ‚úì Bengali
"Hello"         ‚úì English
"ŸÖÿ±ÿ≠ÿ®ÿß"         ‚úì Arabic
"‰Ω†Â•Ω"           ‚úì Chinese
"üî•üöÄ"           ‚úì Emojis
```

**2. Empty Strings:**
```
""   ‚Üí returns ""
```

**3. Unterminated Strings:**
```
"hello
(no closing quote)

Stops at: ch == 0 (EOF)
Returns partial: "hello\n" (including newline)
```

**4. No Escape Sequences (Currently):**
```
"line1\nline2"  ‚Üí literal string, \n not interpreted
```

**Future Enhancement:**
Could add escape sequence handling:
```go
for {
    l.readChar()
    if l.ch == '\\' {
        l.readChar()  // Skip escape and next char
    } else if l.ch == '"' || l.ch == 0 {
        break
    }
}
```

---

## Whitespace and Comments

### skipWhitespace() - Ignore Whitespace

```go
func (l *Lexer) skipWhitespace() {
    for l.ch == ' ' || l.ch == '\t' || l.ch == '\n' || l.ch == '\r' {
        l.readChar()
    }
}
```

#### Whitespace Characters

- `' '` - Space
- `'\t'` - Tab
- `'\n'` - Newline (Line Feed)
- `'\r'` - Carriage Return

**Example:**
```
Input: "‡¶ß‡¶∞‡¶ø    x\t=\n‡ß´;"
        ^^^^  ^^^ (whitespace)

After skipWhitespace():
  Tokens: LET, IDENT, ASSIGN, INT, SEMICOLON
  (Whitespace not in token stream)
```

### skipComment() - Ignore Line Comments

```go
func (l *Lexer) skipComment() {
    for l.ch != '\n' && l.ch != 0 {
        l.readChar()
    }
    l.skipWhitespace()
}
```

#### Comment Processing

**Example:**
```
Input: "‡¶ß‡¶∞‡¶ø x = ‡ß´; // ‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ï‡¶Æ‡ßá‡¶®‡ßç‡¶ü\n‡¶ß‡¶∞‡¶ø y = ‡ßß‡ß¶;"

Position: '‡¶ß' '‡¶∞' '‡¶ø' ' ' 'x' ' ' '=' ' ' '‡ß´' ';' ' ' '/' '/'
                                                             ^
When at '/':
  peekChar() == '/' ‚Üí it's a comment
  skipComment() reads until '\n' or EOF
  skipWhitespace() skips the newline
  NextToken() called recursively
  
Result: Comment ignored, continues with next line
```

**Why Call skipWhitespace() After Comment?**
- Comments end at newline
- Newline is whitespace
- Remove newline to continue cleanly

---

## Unicode Handling

### isLetter() - Check if Character is Letter

```go
func isLetter(ch rune) bool {
    // Check for letters, Bengali vowel signs (‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ), and other combining marks
    return unicode.IsLetter(ch) || ch == '_' || isBengaliVowelSign(ch) || unicode.Is(unicode.Mn, ch)
}
```

#### Why So Complex?

**Problem: Bengali Script is Complex**

Bengali words are not just sequences of letters:

```
Word: ‡¶ï‡¶æ‡¶ú (kaj - work)
  ‡¶ï (k) - base letter
  ‡¶æ (a) - vowel sign (not a separate letter)
  ‡¶ú (j) - base letter
```

If we only checked `unicode.IsLetter()`:
- `‡¶ï` ‚Üí true (letter)
- `‡¶æ` ‚Üí false (not a letter, it's a combining mark)
- Result: Would split at vowel sign!

**Solution: Include Combining Marks**

```go
return unicode.IsLetter(ch) ||        // Regular letters
       ch == '_' ||                   // Underscore (for identifiers)
       isBengaliVowelSign(ch) ||      // Bengali vowel signs
       unicode.Is(unicode.Mn, ch)     // Nonspacing marks (diacritics)
```

### isBengaliVowelSign() - Check Bengali Vowel Sign

```go
func isBengaliVowelSign(ch rune) bool {
    // Bengali vowel signs (‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ) and diacritics: U+0981 to U+09CD
    // This includes: ‡¶Å ‡¶Ç ‡¶É, vowel signs, and hasant
    return (ch >= 0x0981 && ch <= 0x09CD) || ch == 0x09D7
}
```

#### Bengali Unicode Ranges

**U+0981 - U+09CD:**
```
U+0981   ‡¶Å   Candrabindu
U+0982   ‡¶Ç   Anusvara
U+0983   ‡¶É   Visarga
U+0985-U+098C   Vowels (‡¶Ö ‡¶Ü ‡¶á ‡¶à ‡¶â ‡¶ä ‡¶ã ‡ß†)
U+098F-U+0990   Vowels (‡¶è ‡¶ê)
U+0993-U+0994   Vowels (‡¶ì ‡¶î)
U+0995-U+09B9   Consonants (‡¶ï-‡¶π)
U+09BC   ‡¶º   Nukta
U+09BE-U+09C4   Vowel signs (‡¶æ ‡¶ø ‡ßÄ ‡ßÅ ‡ßÇ ‡ßÉ ‡ßá ‡ßà)
U+09C7-U+09C8   Vowel signs (‡ßá ‡ßà)
U+09CB-U+09CC   Vowel signs (‡ßã ‡ßå)
U+09CD   ‡ßç   Hasant (virama)
```

**U+09D7:**
```
U+09D7   ‡ßó   AU length mark
```

#### Example: Complex Bengali Word

```
Word: ‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡ßç‡¶ü (struct)

Character breakdown:
  ‡¶∏   U+09B8  - Base letter
  ‡ßç   U+09CD  - Hasant (conjunct former)
  ‡¶ü   U+099F  - Base letter
  ‡ßç   U+09CD  - Hasant
  ‡¶∞   U+09B0  - Base letter
  ‡¶æ   U+09BE  - Vowel sign
  ‡¶ï   U+0995  - Base letter
  ‡ßç   U+09CD  - Hasant
  ‡¶ü   U+099F  - Base letter

All treated as single identifier by isLetter()
```

### isDigit() and isBengaliDigit()

```go
func isDigit(ch rune) bool {
    return '0' <= ch && ch <= '9'
}

func isBengaliDigit(ch rune) bool {
    return '‡ß¶' <= ch && ch <= '‡ßØ'
}
```

#### Digit Ranges

**Arabic Numerals:** U+0030 - U+0039
```
0 (U+0030) to 9 (U+0039)
```

**Bengali Numerals:** U+09E6 - U+09EF
```
‡ß¶ (U+09E6) to ‡ßØ (U+09EF)
```

---

## Operator Recognition

### Single-Character Operators

Simple operators are recognized directly:

```go
case '+':
    tok = l.newTokenWithPos(token.PLUS, string(l.ch))
case '-':
    tok = l.newTokenWithPos(token.MINUS, string(l.ch))
case '*':
    tok = l.newTokenWithPos(token.ASTERISK, string(l.ch))
case '(':
    tok = l.newTokenWithPos(token.LPAREN, string(l.ch))
// ... etc
```

### Multi-Character Operators

Multi-character operators require lookahead:

#### Equality Operators

```go
case '=':
    if l.peekChar() == '=' {
        // "=="
        ch := l.ch
        l.readChar()  // Consume second '='
        tok = l.newTokenWithPos(token.EQ, string(ch)+string(l.ch))
    } else if l.peekChar() == '>' {
        // "=>"
        ch := l.ch
        l.readChar()  // Consume '>'
        tok = l.newTokenWithPos(token.ARROW, string(ch)+string(l.ch))
    } else {
        // "="
        tok = l.newTokenWithPos(token.ASSIGN, string(l.ch))
    }
```

**Decision Tree:**
```
Current: '='
  ‚îú‚îÄ Peek: '=' ‚Üí Token: EQ ("==")
  ‚îú‚îÄ Peek: '>' ‚Üí Token: ARROW ("=>")
  ‚îî‚îÄ Peek: other ‚Üí Token: ASSIGN ("=")
```

#### Comparison Operators

```go
case '<':
    if l.peekChar() == '=' {
        // "<="
        ch := l.ch
        l.readChar()
        tok = l.newTokenWithPos(token.LTE, string(ch)+string(l.ch))
    } else if l.peekChar() == '<' {
        // "<<"
        ch := l.ch
        l.readChar()
        tok = l.newTokenWithPos(token.LSHIFT, string(ch)+string(l.ch))
    } else {
        // "<"
        tok = l.newTokenWithPos(token.LT, string(l.ch))
    }
```

**Decision Tree:**
```
Current: '<'
  ‚îú‚îÄ Peek: '=' ‚Üí Token: LTE ("<=")
  ‚îú‚îÄ Peek: '<' ‚Üí Token: LSHIFT ("<<")
  ‚îî‚îÄ Peek: other ‚Üí Token: LT ("<")
```

#### Logical and Bitwise Operators

```go
case '&':
    if l.peekChar() == '&' {
        // "&&" (logical AND)
        ch := l.ch
        l.readChar()
        tok = l.newTokenWithPos(token.AND, string(ch)+string(l.ch))
    } else {
        // "&" (bitwise AND)
        tok = l.newTokenWithPos(token.BIT_AND, string(l.ch))
    }
```

**Disambiguation:**
```
'&'  ‚Üí BIT_AND (bitwise)
'&&' ‚Üí AND (logical)

'|'  ‚Üí BIT_OR (bitwise)
'||' ‚Üí OR (logical)
```

### Complete Operator Set

#### Arithmetic Operators
```
+   PLUS
-   MINUS
*   ASTERISK
/   SLASH
%   PERCENT
```

#### Comparison Operators
```
<   LT
>   GT
==  EQ
!=  NOT_EQ
<=  LTE
>=  GTE
```

#### Logical Operators
```
!   BANG (NOT)
&&  AND
||  OR
```

#### Bitwise Operators
```
&   BIT_AND
|   BIT_OR
^   BIT_XOR
~   BIT_NOT
<<  LSHIFT
>>  RSHIFT
```

#### Other Operators
```
=   ASSIGN
=>  ARROW (function arrow)
```

---

## Position Tracking

### Why Track Positions?

Position information enables:
1. **Error Messages**: Show where errors occur
2. **Debugging**: Help developers locate issues
3. **IDE Features**: Hover information, go-to-definition
4. **Syntax Highlighting**: Associate tokens with locations

### Position Fields

```go
type Token struct {
    Type    TokenType
    Literal string
    Line    int     // 1-indexed
    Column  int     // 1-indexed after first character
}
```

### Line Tracking

Lines increment on newline:

```go
if l.ch == '\n' {
    l.line++
    l.column = 0
}
```

**Example:**
```
Input:
Line 1: ‡¶ß‡¶∞‡¶ø x = ‡ß´;
Line 2: ‡¶ß‡¶∞‡¶ø y = ‡ßß‡ß¶;

After "‡ß´;":  line = 1, column = 11
After '\n': line = 2, column = 0
After '‡¶ß':  line = 2, column = 1
```

### Column Tracking

Columns increment on each character:

```go
l.column++  // In readChar()
```

**Column Reset:**
- Starts at 0 (becomes 1 after first readChar())
- Resets to 0 on newline

### Token Position Recording

```go
// Record position BEFORE consuming token
tokLine := l.line
tokCol := l.column

// ... read token ...

tok = token.Token{
    Type:    tokenType,
    Literal: literal,
    Line:    tokLine,   // Start line
    Column:  tokCol,    // Start column
}
```

**Why record before?**
- After reading token, position is at next character
- Token should be labeled with its starting position

### Position Calculation for Multi-Character Tokens

For tokens created with `newTokenWithPos()`:

```go
Column: l.column - len([]rune(literal)) + 1
```

**Example:**
```
Current: "==" at position column = 8
Token started at: 8 - 2 + 1 = 7
```

### Error Reporting Example

```go
// In parser or evaluator
if err != nil {
    fmt.Printf("Error at line %d, column %d: %s\n",
        token.Line, token.Column, err.Error())
    
    // Show source line with error marker
    showErrorLine(source, token.Line, token.Column)
}

func showErrorLine(source string, line, col int) {
    lines := strings.Split(source, "\n")
    if line <= len(lines) {
        fmt.Println(lines[line-1])
        fmt.Println(strings.Repeat(" ", col-1) + "^")
    }
}
```

**Output:**
```
Error at line 3, column 12: unexpected token '!'
    ‡¶ß‡¶∞‡¶ø x = ‡ßß‡ß¶!
               ^
```

---

## Complete Examples

### Example 1: Simple Variable Declaration

**Input:**
```bengali
‡¶ß‡¶∞‡¶ø x = ‡ß´;
```

**Tokenization Trace:**

```
Initial State:
  input = ['‡¶ß', '‡¶∞', '‡¶ø', ' ', 'x', ' ', '=', ' ', '‡ß´', ';']
  position = 0, readPosition = 1
  ch = '‡¶ß', line = 1, column = 1

Call NextToken():
  skipWhitespace() - ch = '‡¶ß' (not whitespace, no change)
  tokLine = 1, tokCol = 1
  isLetter('‡¶ß') = true
  readIdentifier():
    startPos = 0
    Loop: '‡¶ß', '‡¶∞', '‡¶ø' (all letters)
    Stop at ' ' (space)
    position = 3, ch = ' '
    return "‡¶ß‡¶∞‡¶ø"
  LookupIdent("‡¶ß‡¶∞‡¶ø") = LET
  Token{Type: LET, Literal: "‡¶ß‡¶∞‡¶ø", Line: 1, Column: 1}

Call NextToken():
  skipWhitespace() - skip ' ', ch = 'x', position = 4, column = 5
  tokLine = 1, tokCol = 5
  isLetter('x') = true
  readIdentifier():
    startPos = 4
    Loop: 'x'
    Stop at ' '
    position = 5, ch = ' '
    return "x"
  LookupIdent("x") = IDENT
  Token{Type: IDENT, Literal: "x", Line: 1, Column: 5}

Call NextToken():
  skipWhitespace() - skip ' ', ch = '=', position = 6, column = 7
  tokLine = 1, tokCol = 7
  case '=':
    peekChar() = ' ' (not '=' or '>')
    tok = ASSIGN
    readChar() - ch = ' '
  Token{Type: ASSIGN, Literal: "=", Line: 1, Column: 7}

Call NextToken():
  skipWhitespace() - skip ' ', ch = '‡ß´', position = 8, column = 9
  tokLine = 1, tokCol = 9
  isBengaliDigit('‡ß´') = true
  readNumber():
    startPos = 8
    Loop: '‡ß´'
    Stop at ';'
    position = 9, ch = ';'
    result = "‡ß´"
    ConvertBengaliNumber("‡ß´") = "5"
    return "5"
  Token{Type: INT, Literal: "5", Line: 1, Column: 9}

Call NextToken():
  skipWhitespace() - ch = ';' (not whitespace)
  tokLine = 1, tokCol = 10
  case ';':
    tok = SEMICOLON
    readChar() - ch = 0 (EOF)
  Token{Type: SEMICOLON, Literal: ";", Line: 1, Column: 10}

Call NextToken():
  skipWhitespace() - ch = 0 (EOF, no change)
  tokLine = 1, tokCol = 11
  case 0:
    tok = EOF
  Token{Type: EOF, Literal: "", Line: 1, Column: 11}
```

**Final Token Stream:**
```
[
  Token{LET, "‡¶ß‡¶∞‡¶ø", 1, 1},
  Token{IDENT, "x", 1, 5},
  Token{ASSIGN, "=", 1, 7},
  Token{INT, "5", 1, 9},
  Token{SEMICOLON, ";", 1, 10},
  Token{EOF, "", 1, 11}
]
```

### Example 2: Function Definition

**Input:**
```bengali
‡¶ß‡¶∞‡¶ø ‡¶Ø‡ßã‡¶ó = ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®(a, b) {
    ‡¶´‡ßá‡¶∞‡¶§ a + b;
};
```

**Token Stream:**
```
Token{LET, "‡¶ß‡¶∞‡¶ø", 1, 1}
Token{IDENT, "‡¶Ø‡ßã‡¶ó", 1, 5}
Token{ASSIGN, "=", 1, 9}
Token{FUNCTION, "‡¶´‡¶æ‡¶Ç‡¶∂‡¶®", 1, 11}
Token{LPAREN, "(", 1, 17}
Token{IDENT, "a", 1, 18}
Token{COMMA, ",", 1, 19}
Token{IDENT, "b", 1, 21}
Token{RPAREN, ")", 1, 22}
Token{LBRACE, "{", 1, 24}
Token{RETURN, "‡¶´‡ßá‡¶∞‡¶§", 2, 5}
Token{IDENT, "a", 2, 10}
Token{PLUS, "+", 2, 12}
Token{IDENT, "b", 2, 14}
Token{SEMICOLON, ";", 2, 15}
Token{RBRACE, "}", 3, 1}
Token{SEMICOLON, ";", 3, 2}
Token{EOF, "", 3, 3}
```

### Example 3: Conditional with Comparison

**Input:**
```bengali
‡¶Ø‡¶¶‡¶ø (x >= ‡ßß‡ß¶) {
    ‡¶≤‡ßá‡¶ñ("‡¶¨‡¶°‡¶º");
} ‡¶®‡¶æ‡¶π‡¶≤‡ßá {
    ‡¶≤‡ßá‡¶ñ("‡¶õ‡ßã‡¶ü");
}
```

**Token Stream:**
```
Token{IF, "‡¶Ø‡¶¶‡¶ø", 1, 1}
Token{LPAREN, "(", 1, 5}
Token{IDENT, "x", 1, 6}
Token{GTE, ">=", 1, 8}
Token{INT, "10", 1, 11}
Token{RPAREN, ")", 1, 13}
Token{LBRACE, "{", 1, 15}
Token{IDENT, "‡¶≤‡ßá‡¶ñ", 2, 5}
Token{LPAREN, "(", 2, 8}
Token{STRING, "‡¶¨‡¶°‡¶º", 2, 9}
Token{RPAREN, ")", 2, 13}
Token{SEMICOLON, ";", 2, 14}
Token{RBRACE, "}", 3, 1}
Token{ELSE, "‡¶®‡¶æ‡¶π‡¶≤‡ßá", 3, 3}
Token{LBRACE, "{", 3, 9}
Token{IDENT, "‡¶≤‡ßá‡¶ñ", 4, 5}
Token{LPAREN, "(", 4, 8}
Token{STRING, "‡¶õ‡ßã‡¶ü", 4, 9}
Token{RPAREN, ")", 4, 13}
Token{SEMICOLON, ";", 4, 14}
Token{RBRACE, "}", 5, 1}
Token{EOF, "", 5, 2}
```

### Example 4: Comments and Bitwise Operations

**Input:**
```bengali
// ‡¶¨‡¶ø‡¶ü‡¶ì‡¶Ø‡¶º‡¶æ‡¶á‡¶ú ‡¶Ö‡¶™‡¶æ‡¶∞‡ßá‡¶∂‡¶®
‡¶ß‡¶∞‡¶ø a = ‡ß´ & ‡ß©;   // AND
‡¶ß‡¶∞‡¶ø b = ‡ß´ | ‡ß©;   // OR
‡¶ß‡¶∞‡¶ø c = ‡ß´ << ‡ß®;  // Left shift
```

**Token Stream:**
```
// First comment skipped
Token{LET, "‡¶ß‡¶∞‡¶ø", 2, 1}
Token{IDENT, "a", 2, 5}
Token{ASSIGN, "=", 2, 7}
Token{INT, "5", 2, 9}
Token{BIT_AND, "&", 2, 11}
Token{INT, "3", 2, 13}
Token{SEMICOLON, ";", 2, 14}
// Second comment skipped
Token{LET, "‡¶ß‡¶∞‡¶ø", 3, 1}
Token{IDENT, "b", 3, 5}
Token{ASSIGN, "=", 3, 7}
Token{INT, "5", 3, 9}
Token{BIT_OR, "|", 3, 11}
Token{INT, "3", 3, 13}
Token{SEMICOLON, ";", 3, 14}
// Third comment skipped
Token{LET, "‡¶ß‡¶∞‡¶ø", 4, 1}
Token{IDENT, "c", 4, 5}
Token{ASSIGN, "=", 4, 7}
Token{INT, "5", 4, 9}
Token{LSHIFT, "<<", 4, 11}
Token{INT, "2", 4, 14}
Token{SEMICOLON, ";", 4, 15}
Token{EOF, "", 4, 16}
```

---

## Testing Strategies

### Unit Tests

Test individual functions:

```go
func TestNew(t *testing.T) {
    input := "‡¶ß‡¶∞‡¶ø"
    l := lexer.New(input)
    
    if l.ch != '‡¶ß' {
        t.Errorf("Expected first char '‡¶ß', got %c", l.ch)
    }
    if l.line != 1 {
        t.Errorf("Expected line 1, got %d", l.line)
    }
}

func TestReadIdentifier(t *testing.T) {
    input := "‡¶´‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü‡ßã‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶≤ = "
    l := lexer.New(input)
    
    ident := l.readIdentifier()
    expected := "‡¶´‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü‡ßã‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶≤"
    
    if ident != expected {
        t.Errorf("Expected %s, got %s", expected, ident)
    }
}
```

### Integration Tests

Test complete tokenization:

```go
func TestNextToken(t *testing.T) {
    input := `‡¶ß‡¶∞‡¶ø x = ‡ß´;`
    
    tests := []struct {
        expectedType    token.TokenType
        expectedLiteral string
        expectedLine    int
        expectedColumn  int
    }{
        {token.LET, "‡¶ß‡¶∞‡¶ø", 1, 1},
        {token.IDENT, "x", 1, 5},
        {token.ASSIGN, "=", 1, 7},
        {token.INT, "5", 1, 9},
        {token.SEMICOLON, ";", 1, 10},
        {token.EOF, "", 1, 11},
    }
    
    l := lexer.New(input)
    
    for i, tt := range tests {
        tok := l.NextToken()
        
        if tok.Type != tt.expectedType {
            t.Fatalf("tests[%d] - wrong type. expected=%q, got=%q",
                i, tt.expectedType, tok.Type)
        }
        if tok.Literal != tt.expectedLiteral {
            t.Fatalf("tests[%d] - wrong literal. expected=%q, got=%q",
                i, tt.expectedLiteral, tok.Literal)
        }
        if tok.Line != tt.expectedLine {
            t.Fatalf("tests[%d] - wrong line. expected=%d, got=%d",
                i, tt.expectedLine, tok.Line)
        }
        if tok.Column != tt.expectedColumn {
            t.Fatalf("tests[%d] - wrong column. expected=%d, got=%d",
                i, tt.expectedColumn, tok.Column)
        }
    }
}
```

### Edge Cases to Test

1. **Empty Input:**
```go
input := ""
// Should return EOF immediately
```

2. **Only Whitespace:**
```go
input := "   \t\n  "
// Should skip all whitespace, return EOF
```

3. **Unterminated String:**
```go
input := `"hello`
// Should handle gracefully (stops at EOF)
```

4. **Long Identifiers:**
```go
input := strings.Repeat("a", 1000)
// Should handle without issues
```

5. **All Bengali Digits:**
```go
input := "‡ß¶‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ"
// Should convert correctly
```

6. **Mixed Scripts:**
```go
input := "variable_‡¶®‡¶æ‡¶Æ"
// Should accept mixed English/Bengali
```

7. **Complex Conjuncts:**
```go
input := "‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡ßç‡¶ü"
// Should handle as single identifier
```

8. **All Operators:**
```go
input := "+ - * / % < > <= >= == != && || & | ^ ~ << >> ! ="
// Should recognize each operator
```

---

## Performance Considerations

### Time Complexity

**Overall: O(n)** where n = input length
- Single pass through input
- Each character read once
- Constant time per character

**Per-Function:**
- `readChar()`: O(1)
- `peekChar()`: O(1)
- `readIdentifier()`: O(m) where m = identifier length
- `readNumber()`: O(m) where m = number length
- `readString()`: O(m) where m = string length
- `skipWhitespace()`: O(m) where m = whitespace count
- `NextToken()`: O(1) amortized

### Space Complexity

**O(n)** where n = input length
- `input` slice: O(n)
- Other fields: O(1)
- No additional data structures

### Optimizations

1. **Rune Slice:**
   - Convert once at initialization
   - Random access in O(1)
   - No repeated conversions

2. **Single Pass:**
   - No backtracking
   - No re-reading
   - Efficient for large files

3. **Peek Without Consume:**
   - Lookahead doesn't advance
   - Avoids buffering

4. **Minimal String Operations:**
   - Slice directly from rune array
   - Avoid repeated string concatenation

### Benchmarking

```go
func BenchmarkNextToken(b *testing.B) {
    input := `‡¶ß‡¶∞‡¶ø ‡¶´‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü‡ßã‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶≤ = ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®(n) {
        ‡¶Ø‡¶¶‡¶ø (n <= ‡ßß) {
            ‡¶´‡ßá‡¶∞‡¶§ ‡ßß;
        }
        ‡¶´‡ßá‡¶∞‡¶§ n * ‡¶´‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü‡ßã‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶≤(n - ‡ßß);
    };`
    
    b.ResetTimer()
    for i := 0; i < b.N; i++ {
        l := lexer.New(input)
        for {
            tok := l.NextToken()
            if tok.Type == token.EOF {
                break
            }
        }
    }
}
```

**Expected Results:**
- Thousands of tokenizations per millisecond
- Linear scaling with input size
- Consistent performance across different character types

---

## Summary

The **Bhasa Lexer** is a robust, Unicode-aware lexical analyzer that:

### Key Features ‚úÖ
- **Unicode Support**: Full Bengali script support using runes
- **Dual Numeral Systems**: Arabic (0-9) and Bengali (‡ß¶-‡ßØ) digits
- **Position Tracking**: Line and column numbers for error reporting
- **Peek-Ahead**: Lookahead for multi-character operators
- **Comment Support**: Line comments with `//`
- **Comprehensive Operators**: Arithmetic, logical, bitwise, comparison
- **Bengali Keywords**: 30+ keywords in Bengali

### Design Principles ‚úÖ
- **Single Pass**: O(n) time complexity
- **No Backtracking**: Efficient character consumption
- **Clear Separation**: Only tokenization, no parsing
- **Maintainable**: Well-structured, easy to extend

### Token Categories ‚úÖ
1. Keywords (Bengali): ‡¶ß‡¶∞‡¶ø, ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®, ‡¶Ø‡¶¶‡¶ø, etc.
2. Identifiers: Variable and function names
3. Literals: Integers and strings
4. Operators: +, -, *, /, ==, !=, &&, ||, etc.
5. Delimiters: (), {}, [], ,, ;, :
6. Special: EOF, ILLEGAL

### Use Cases ‚úÖ
- **Parser Input**: Provides token stream for syntax analysis
- **Error Detection**: Identifies invalid characters
- **Position Tracking**: Enables precise error messages
- **Syntax Highlighting**: Provides token types for IDE features

The lexer forms the foundation of the Bhasa interpreter, converting raw text into a structured token stream ready for parsing and evaluation.

---

**Next Steps:**
- Parser uses token stream to build AST
- Evaluator interprets AST to execute code
- Compiler (optional) generates bytecode

For parser documentation, see `../parser/docs/`.

